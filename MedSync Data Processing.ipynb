{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weill Cornell MedSync Data Processing\n",
    "## How might we recognize handwritten medical charts and convert them to searchable text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import enchant\n",
    "import string\n",
    "import pandas as pd\n",
    "import docx\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "containsTranscriptions = []\n",
    "noTranscriptions = []\n",
    "docNames = []\n",
    "\n",
    "def getDocuments2(loc, f0, f1, f2) :\n",
    "    if f2.endswith(\".docx\") :\n",
    "        docNames.append(f2)\n",
    "    elif f2 == \"Completed Transcriptions\" :\n",
    "        for f3 in os.listdir(loc + f0 + \"/\" + f1 + \"/\" + f2) :\n",
    "            if f3.endswith(\".docx\") :\n",
    "                docNames.append(f3)\n",
    "\n",
    "def getDocuments1(loc, f0, f1) :\n",
    "    if f1 == \"Transcribed Documents\" :\n",
    "        noTranscriptions.remove(f0)\n",
    "        containsTranscriptions.append(f0)\n",
    "        for f2 in os.listdir(loc + f0 + \"/\" + f1) :\n",
    "            getDocuments2(loc, f0, f1, f2)\n",
    "\n",
    "def getDocuments0(loc) :\n",
    "    for f0 in os.listdir(loc) :\n",
    "        if f0 != \".DS_Store\" :\n",
    "            noTranscriptions.append(f0)\n",
    "            for f1 in os.listdir(loc + f0) :\n",
    "                getDocuments1(loc, f0, f1)\n",
    "\n",
    "def getDocuments00(loc) :\n",
    "    titles = [] ; words = []\n",
    "    for f in os.listdir(loc) :\n",
    "        if f.endswith(\".txt\") :\n",
    "            titles.append(f.replace(' copy.txt',''))\n",
    "            f1 = open(\"TranscribedFolder/\" + f)\n",
    "            words.append(f1.read().replace('\\n', ''))\n",
    "    \n",
    "    return titles, words\n",
    "    \n",
    "getDocuments0(\"Transcriptions/\")\n",
    "allTitles, allWords = getDocuments00(\"TranscribedFolder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Titles are consistent\n",
    "# t10 = [] ; t20 = []\n",
    "# for f in os.listdir(\"TranscriptionImages/\") :\n",
    "#     if f.endswith(\".png\") :\n",
    "#         t10.append(f.replace('.png',''))\n",
    "\n",
    "# print(list(set(allTitles) - set(t10)))\n",
    "# print(list(set(t10) - set(allTitles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transcriber 01',\n",
       " 'Transcriber 02',\n",
       " 'Transcriber 03',\n",
       " 'Transcriber 04',\n",
       " 'Transcriber 05',\n",
       " 'Transcriber 06',\n",
       " 'Transcriber 07',\n",
       " 'Transcriber 08',\n",
       " 'Transcriber 09',\n",
       " 'Transcriber 10',\n",
       " 'Transcriber 11',\n",
       " 'Transcriber 12',\n",
       " 'Transcriber 13',\n",
       " 'Transcriber 14',\n",
       " 'Transcriber 16',\n",
       " 'Transcriber 17',\n",
       " 'Transcriber 18']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containsTranscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transcriber 15',\n",
       " 'Transcriber 19',\n",
       " 'Transcriber 20',\n",
       " 'Transcriber 21',\n",
       " 'Transcriber 22',\n",
       " 'Transcriber 23',\n",
       " 'Transcriber 24']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noTranscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NYH Med Div 1837_39_p311.docx',\n",
       " 'NYH Med Div 1850_51_P5.docx',\n",
       " 'NYH med div 1850_51_p526.docx',\n",
       " 'NYH Med Div 1850_51_pg12.docx',\n",
       " 'Medical_1847-48_P335.docx',\n",
       " 'Medical_1855-1856_P2.docx',\n",
       " 'Medical_1855-56_P125.docx',\n",
       " 'Medical_1855-56_P126.docx',\n",
       " 'Medical_New York Hospital 1st Surgical Division Casebook_1840-42_P5.docx',\n",
       " '1st_Surgical_1846-47 Part One P4.docx',\n",
       " '1st_Surgical_1846-47 Part One P46.docx',\n",
       " '1st_Surgical_1846-47 Part One P88.docx',\n",
       " '1st_Surgical_1846-47 Part One P89.docx',\n",
       " '1st_Surgical_1850-51_P108.docx',\n",
       " '1st_Surgical_1850_P2.docx',\n",
       " '1t_Surgical_1850-51_P69.docx',\n",
       " '1st_Surgical _1854-1855_P235.docx',\n",
       " '1st_Surgical _1854-1855_P301.docx',\n",
       " '1st_Surgical Casebook_1857-58_p116.docx',\n",
       " '1st_Surgical Casebook_1857-58_P172.docx',\n",
       " '1st_Surgical Casebook_1857-58_P3.docx',\n",
       " '1st_Surgical Casebook_1857_58_P527.docx',\n",
       " '2nd_Medical_1862-63_P121.docx',\n",
       " '2nd_Medical_1862-63_P122.docx',\n",
       " '2nd_Medical_1862-63_P2.docx',\n",
       " '2nd_Medical_1862-63_P3.docx',\n",
       " '2nd_Surgical_1849-50-P31.docx',\n",
       " '2nd_Surgical_1849-50_P135.docx',\n",
       " '2nd_Surgical_1849-50_P2.docx',\n",
       " 'Medicial_1841-42.P17.docx',\n",
       " 'Medicial_1841-42.P18.docx',\n",
       " 'Medicial_1841-42.P19.docx',\n",
       " 'Medicial_1841-42.P20.docx',\n",
       " 'Medical Division Casebook_1841-42_P140.docx',\n",
       " 'Medical Division Casebook_1846-47_P1.docx',\n",
       " 'New York Hospital Medical Division Casebook_1846-47_P247.docx',\n",
       " 'New York Hospital Medical Division Casebook_1846-47_P303.docx',\n",
       " 'New York Hospital Medical Division Casebook_1846-47_P304.docx',\n",
       " 'New York Hospital Medical Division Casebook_1856_P6.docx',\n",
       " 'Medical_1856_P115.docx',\n",
       " 'Medical_1856_P136.docx',\n",
       " 'medical_1856_P403.docx',\n",
       " 'Medical_1856_P71.docx',\n",
       " 'Medical Cases_1862_P179.docx',\n",
       " 'Medical Cases_1862_P52.docx',\n",
       " 'Medical Cases_1862_P53.docx',\n",
       " 'New York Hospital Medical Division Casebook_1862_P178.docx',\n",
       " 'MEDICAL CASES_1844-45_P415.docx',\n",
       " 'MEDICAL CASES_1844-45_P416.docx',\n",
       " 'Surgical_1837-39_P262.docx']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean and Store Data\n",
    "\n",
    "Medical dictionary from the [Pacific Northwest University of Health Sciences](http://www.pnwu.edu/inside-pnwu/departments/technology-resources/medical-dictionary/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get English dictionary from PyEnchant library\n",
    "english = enchant.Dict(\"en_us\")\n",
    "# Get Medical dictionary\n",
    "f1 = open(\"medicalVocabulary.txt\")\n",
    "medicalVocab = f1.read() ; f1.close()\n",
    "medicalVocab = medicalVocab.lower() ; medicalVocab = medicalVocab.split(\"\\n\") \n",
    "# Get Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Function to check if String contains number\n",
    "def digitExists(data):\n",
    "    return any(x.isdigit() for x in data)\n",
    "\n",
    "# Strip punctuation function\n",
    "def stripPunctuation(term) :\n",
    "    punctuation = list(string.punctuation) # String of punctuation characters\n",
    "    punctuation.remove('.') # To prevent issues with ellipses.\n",
    "    whiteList = ['â€”', '.'] # We want to replace em-dashes and ellipses with whitespace\n",
    "    punctuation.append(\"+\") ; punctuation.append(\"-\") ; punctuation.append(\".\")\n",
    "    stripped = ''\n",
    "    for character in term:\n",
    "        if character not in punctuation and character in whiteList:\n",
    "            stripped = stripped + ' '\n",
    "        elif character not in punctuation and character not in whiteList:\n",
    "            stripped = stripped + character\n",
    "    return stripped\n",
    "\n",
    "# Function to clean list of Strings into lists of lists of tokens\n",
    "def clean(w):\n",
    "    cleaned = []\n",
    "    for doc in w:\n",
    "        temp = []\n",
    "        for term in doc.split() :\n",
    "            if term not in stop and len(term) > 4  and (digitExists(term) == False) :\n",
    "                temp.append(stripPunctuation(term.lower()))\n",
    "        cleaned.append(temp)\n",
    "    return cleaned\n",
    "\n",
    "# Function that takes in a single document, the list of non-English words,\n",
    "# the list of medical terms, and the vocabulary of English words from document\n",
    "def process(words) :\n",
    "    totalV = []; error = [] ; medical = []\n",
    "    for l in words:\n",
    "        for word in l:\n",
    "            if word : # Check if String is not empty\n",
    "                val = int(english.check(word))\n",
    "                if word in medicalVocab : # Check if word is medical\n",
    "                    medical.append(word)\n",
    "                elif english.check(word) :\n",
    "                    totalV.append(word)   \n",
    "                else :\n",
    "                    error.append(word)\n",
    "    # Remove reundant words\n",
    "    totalV = list(set(totalV))\n",
    "    medical = sorted(list(set(medical)))\n",
    "    error = list(set(error))\n",
    "    \n",
    "    return totalV, medical, error\n",
    "\n",
    "# Function to generate Pandas DataFrame from processed data\n",
    "def generateDF(titles, documents) :\n",
    "    df = pd.DataFrame(columns = ('File', 'Words'))\n",
    "    for i in range(0, len(titles)) :\n",
    "        df.loc[i] = [titles[i], documents[i]]\n",
    "        df.loc[i] = [titles[i], ' '.join(documents[i])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allCleaned = clean(allWords)\n",
    "voc, medical, error = process(allCleaned)\n",
    "df = generateDF(allTitles, allCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Data (Bag-of-Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary = voc, min_df = 50)\n",
    "allCounts = cv.fit_transform(df['Words'].values)\n",
    "trainDF = pd.DataFrame(allCounts.todense(), columns = cv.get_feature_names())\n",
    "wordCounts = pd.concat([df['File'], trainDF], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordCounts[wordCounts != 0].count()\n",
    "wc = wc.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCounts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(vocabulary = medical, min_df = 0.3)\n",
    "mCounts = cv1.fit_transform(df['Words'].values)\n",
    "medDF = pd.DataFrame(mCounts.todense(), columns = cv1.get_feature_names())\n",
    "medicalCounts = pd.concat([df['File'], medDF], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"Output/\")\n",
    "wordCounts.to_csv('allCounts.csv', encoding='utf-8', index=False)\n",
    "medicalCounts.to_csv('medicalCounts.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
