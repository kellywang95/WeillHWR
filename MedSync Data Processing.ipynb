{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weill Cornell MedSync Data Processing\n",
    "## How might we recognize handwritten medical charts and convert them to searchable text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import enchant\n",
    "import string\n",
    "import pandas as pd\n",
    "import docx\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "containsTranscriptions = []\n",
    "noTranscriptions = []\n",
    "docNames = []\n",
    "\n",
    "def getDocuments2(loc, f0, f1, f2) :\n",
    "    if f2.endswith(\".docx\") :\n",
    "        docNames.append(f2)\n",
    "    elif f2 == \"Completed Transcriptions\" :\n",
    "        for f3 in os.listdir(loc + f0 + \"/\" + f1 + \"/\" + f2) :\n",
    "            if f3.endswith(\".docx\") :\n",
    "                docNames.append(f3)\n",
    "\n",
    "def getDocuments1(loc, f0, f1) :\n",
    "    if f1 == \"Transcribed Documents\" :\n",
    "        noTranscriptions.remove(f0)\n",
    "        containsTranscriptions.append(f0)\n",
    "        for f2 in os.listdir(loc + f0 + \"/\" + f1) :\n",
    "            getDocuments2(loc, f0, f1, f2)\n",
    "\n",
    "def getDocuments0(loc) :\n",
    "    for f0 in os.listdir(loc) :\n",
    "        if f0 != \".DS_Store\" :\n",
    "            noTranscriptions.append(f0)\n",
    "            for f1 in os.listdir(loc + f0) :\n",
    "                getDocuments1(loc, f0, f1)\n",
    "\n",
    "def getDocuments00(loc) :\n",
    "    titles = [] ; words = []\n",
    "    for f in os.listdir(loc) :\n",
    "        if f.endswith(\".txt\") :\n",
    "            titles.append(f.replace(' copy.txt',''))\n",
    "            f1 = open(\"TranscribedFolder/\" + f)\n",
    "            words.append(f1.read().replace('\\n', ''))\n",
    "    \n",
    "    return titles, words\n",
    "    \n",
    "# getDocuments0(\"Transcriptions/\")\n",
    "allTitles, allWords = getDocuments00(\"TranscribedFolder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure Titles are consistent\n",
    "# t10 = [] ; t20 = []\n",
    "# for f in os.listdir(\"TranscriptionImages/\") :\n",
    "#     if f.endswith(\".png\") :\n",
    "#         t10.append(f.replace('.png',''))\n",
    "\n",
    "# print(list(set(allTitles) - set(t10)))\n",
    "# print(list(set(t10) - set(allTitles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noTranscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean and Store Data\n",
    "\n",
    "Medical dictionary from the [Pacific Northwest University of Health Sciences](http://www.pnwu.edu/inside-pnwu/departments/technology-resources/medical-dictionary/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English dictionary from PyEnchant library\n",
    "english = enchant.Dict(\"en_us\")\n",
    "# Get Medical dictionary\n",
    "f1 = open(\"medicalVocabulary.txt\")\n",
    "medicalVocab = f1.read() ; f1.close()\n",
    "medicalVocab = medicalVocab.lower() ; medicalVocab = medicalVocab.split(\"\\n\") \n",
    "# Get Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Function to check if String contains number\n",
    "def digitExists(data):\n",
    "    return any(x.isdigit() for x in data)\n",
    "\n",
    "# Strip punctuation function\n",
    "def stripPunctuation(term) :\n",
    "    punctuation = list(string.punctuation) # String of punctuation characters\n",
    "    punctuation.remove('.') # To prevent issues with ellipses.\n",
    "    whiteList = ['â€”', '.'] # We want to replace em-dashes and ellipses with whitespace\n",
    "    punctuation.append(\"+\") ; punctuation.append(\"-\") ; punctuation.append(\".\")\n",
    "    stripped = ''\n",
    "    for character in term:\n",
    "        if character not in punctuation and character in whiteList:\n",
    "            stripped = stripped + ' '\n",
    "        elif character not in punctuation and character not in whiteList:\n",
    "            stripped = stripped + character\n",
    "    return stripped\n",
    "\n",
    "# Function to clean list of Strings into lists of lists of tokens\n",
    "def clean(w):\n",
    "    cleaned = []\n",
    "    for doc in w:\n",
    "        temp = []\n",
    "        for term in doc.split() :\n",
    "            if term not in stop and len(term) > 4  and (digitExists(term) == False) :\n",
    "                temp.append(stripPunctuation(term.lower()))\n",
    "        cleaned.append(temp)\n",
    "    return cleaned\n",
    "\n",
    "# Function that takes in a single document, the list of non-English words,\n",
    "# the list of medical terms, and the vocabulary of English words from document\n",
    "def process(words) :\n",
    "    totalV = []; error = [] ; medical = []\n",
    "    for l in words:\n",
    "        for word in l:\n",
    "            if word : # Check if String is not empty\n",
    "                val = int(english.check(word))\n",
    "                if word in medicalVocab : # Check if word is medical\n",
    "                    medical.append(word)\n",
    "                elif english.check(word) :\n",
    "                    totalV.append(word)   \n",
    "                else :\n",
    "                    error.append(word)\n",
    "    # Remove reundant words\n",
    "    totalV = list(set(totalV))\n",
    "    medical = sorted(list(set(medical)))\n",
    "    error = list(set(error))\n",
    "    \n",
    "    return totalV, medical, error\n",
    "\n",
    "# Function to generate Pandas DataFrame from processed data\n",
    "def generateDF(titles, documents) :\n",
    "    df = pd.DataFrame(columns = ('File', 'Words'))\n",
    "    for i in range(0, len(titles)) :\n",
    "        df.loc[i] = [titles[i], documents[i]]\n",
    "        df.loc[i] = [titles[i], ' '.join(documents[i])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allCleaned = clean(allWords)\n",
    "voc, medical, error = process(allCleaned)\n",
    "df = generateDF(allTitles, allCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st_Surgical _1854-1855_P235</td>\n",
       "      <td>inflamation ankle joint catherine lundy marrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st_Surgical _1854-1855_P301</td>\n",
       "      <td>poisoned sarah miller single amdnov markel adm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st_Surgical Casebook_1857-58_p116</td>\n",
       "      <td>spinal disease haine sep gauburen unfortunate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st_Surgical Casebook_1857-58_P172</td>\n",
       "      <td>wound chest tityen germany bartendersep  patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st_Surgical Casebook_1857-58_P3</td>\n",
       "      <td>iliac abscess horton ireland laborer admjuly p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 File  \\\n",
       "0        1st_Surgical _1854-1855_P235   \n",
       "1        1st_Surgical _1854-1855_P301   \n",
       "2  1st_Surgical Casebook_1857-58_p116   \n",
       "3  1st_Surgical Casebook_1857-58_P172   \n",
       "4    1st_Surgical Casebook_1857-58_P3   \n",
       "\n",
       "                                               Words  \n",
       "0  inflamation ankle joint catherine lundy marrie...  \n",
       "1  poisoned sarah miller single amdnov markel adm...  \n",
       "2  spinal disease haine sep gauburen unfortunate ...  \n",
       "3  wound chest tityen germany bartendersep  patie...  \n",
       "4  iliac abscess horton ireland laborer admjuly p...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Data (Bag-of-Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary = voc, min_df = 50)\n",
    "allCounts = cv.fit_transform(df['Words'].values)\n",
    "trainDF = pd.DataFrame(allCounts.todense(), columns = cv.get_feature_names())\n",
    "wordCounts = pd.concat([df['File'], trainDF], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc = wordCounts[wordCounts != 0].count()\n",
    "wc = wc.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File            50\n",
       "pulse           27\n",
       "patient         25\n",
       "since           24\n",
       "right           22\n",
       "admission       21\n",
       "discharged      19\n",
       "tongue          18\n",
       "treatment       17\n",
       "still           16\n",
       "bowels          15\n",
       "night           15\n",
       "slight          15\n",
       "appetite        14\n",
       "cured           13\n",
       "cough           13\n",
       "pains           13\n",
       "considerable    13\n",
       "taken           12\n",
       "first           11\n",
       "fever           11\n",
       "weeks           11\n",
       "better          11\n",
       "oz              10\n",
       "rather          10\n",
       "three           10\n",
       "chill           10\n",
       "there           10\n",
       "moist            9\n",
       "removed          9\n",
       "                ..\n",
       "suite            1\n",
       "warmth           1\n",
       "cocks            1\n",
       "frost            1\n",
       "median           1\n",
       "afforded         1\n",
       "distant          1\n",
       "death            1\n",
       "drank            1\n",
       "tinge            1\n",
       "scattered        1\n",
       "gammon           1\n",
       "shape            1\n",
       "moderate         1\n",
       "depth            1\n",
       "wheels           1\n",
       "become           1\n",
       "die              1\n",
       "enlarged         1\n",
       "inflicting       1\n",
       "vegetable        1\n",
       "d                0\n",
       "c                0\n",
       "m                0\n",
       "l                0\n",
       "f                0\n",
       "s                0\n",
       "w                0\n",
       "z                0\n",
       "a                0\n",
       "Length: 1250, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordCounts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(vocabulary = medical, min_df = 0.3)\n",
    "mCounts = cv1.fit_transform(df['Words'].values)\n",
    "medDF = pd.DataFrame(mCounts.todense(), columns = cv1.get_feature_names())\n",
    "medicalCounts = pd.concat([df['File'], medDF], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"Output/\")\n",
    "wordCounts.to_csv('allCounts.csv', encoding='utf-8', index=False)\n",
    "medicalCounts.to_csv('medicalCounts.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
